{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T08:26:49.128989Z","iopub.execute_input":"2025-01-16T08:26:49.129294Z","iopub.status.idle":"2025-01-16T08:26:53.463066Z","shell.execute_reply.started":"2025-01-16T08:26:49.129257Z","shell.execute_reply":"2025-01-16T08:26:53.461752Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torchdata torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T08:26:53.465062Z","iopub.execute_input":"2025-01-16T08:26:53.465470Z","iopub.status.idle":"2025-01-16T08:26:57.698031Z","shell.execute_reply.started":"2025-01-16T08:26:53.465429Z","shell.execute_reply":"2025-01-16T08:26:57.696827Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading libraries\n1. load_datasets\n2. AutoModelForSeq2SeqLM\n3. AutoTokenizer\n4. GenerationConfig","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM # to load pretrained s2s model like T5 , BASRT etc\nfrom transformers import AutoTokenizer # to tokenize\nfrom transformers import GenerationConfig # to play with temperature ,top p topk,blah blah","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T08:32:47.001450Z","iopub.execute_input":"2025-01-16T08:32:47.002279Z","iopub.status.idle":"2025-01-16T08:32:56.159280Z","shell.execute_reply.started":"2025-01-16T08:32:47.002231Z","shell.execute_reply":"2025-01-16T08:32:56.158196Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nds = load_dataset(\"knkarthick/dialogsum\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T08:27:16.500688Z","iopub.execute_input":"2025-01-16T08:27:16.501467Z","iopub.status.idle":"2025-01-16T08:27:19.326528Z","shell.execute_reply.started":"2025-01-16T08:27:16.501421Z","shell.execute_reply":"2025-01-16T08:27:19.325734Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Summarize Dialogue without Prompt Enginnering","metadata":{}},{"cell_type":"markdown","source":"using pre trained LLM **FLAN-T5** from Huggingface .abs\nOther models of any category(Image ,video ,LLM) can be found https://huggingface.co/docs/transformers/index","metadata":{}},{"cell_type":"markdown","source":"### Visualising the dataset","metadata":{}},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T08:41:37.125171Z","iopub.execute_input":"2025-01-16T08:41:37.125659Z","iopub.status.idle":"2025-01-16T08:41:37.133438Z","shell.execute_reply.started":"2025-01-16T08:41:37.125618Z","shell.execute_reply":"2025-01-16T08:41:37.132092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training \ntrain_dataset =ds[\"train\"]\ntrain_dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T08:44:42.355590Z","iopub.execute_input":"2025-01-16T08:44:42.356057Z","iopub.status.idle":"2025-01-16T08:44:42.363576Z","shell.execute_reply.started":"2025-01-16T08:44:42.356020Z","shell.execute_reply":"2025-01-16T08:44:42.362382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Number of data in training is\" , len(train_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T08:45:10.168260Z","iopub.execute_input":"2025-01-16T08:45:10.168732Z","iopub.status.idle":"2025-01-16T08:45:10.175278Z","shell.execute_reply.started":"2025-01-16T08:45:10.168701Z","shell.execute_reply":"2025-01-16T08:45:10.173958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\ndef print_border_line():\n    dash_line =\"-\".join(\"\" for x in range(60))\n    print(dash_line)\n\ndef print_random_conversation():\n    rndm_convo = random.choice(train_dataset)\n    dialogue = rndm_convo[\"dialogue\"]\n    summary = rndm_convo[\"summary\"]\n    print(\"Dialogue\\n\",dialogue)\n    print_border_line()\n    print(\"Summary\\n\",summary)\n\nprint_random_conversation()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T08:51:04.137457Z","iopub.execute_input":"2025-01-16T08:51:04.137903Z","iopub.status.idle":"2025-01-16T08:51:04.147340Z","shell.execute_reply.started":"2025-01-16T08:51:04.137868Z","shell.execute_reply":"2025-01-16T08:51:04.145922Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"just using pre trained model no fine tuning , so choosing any of train ,test.validation to do prompt engineering","metadata":{}},{"cell_type":"code","source":"example_indices=[40,200]\n\ntest_ds = ds[\"test\"]\n\nfor i, index in enumerate(example_indices):\n    print_border_line()\n    print(\"Example Number \",i+1)\n    print_border_line()\n    print(\"Input Dialogue: \")\n    print(test_ds[index][\"dialogue\"])\n    print_border_line()\n    print(\"BASELINE HUMAN SUMMARY:\")\n    print(test_ds[index][\"summary\"])\n    print_border_line()\n    print(\"\\n\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T08:55:11.730433Z","iopub.execute_input":"2025-01-16T08:55:11.730987Z","iopub.status.idle":"2025-01-16T08:55:11.745057Z","shell.execute_reply.started":"2025-01-16T08:55:11.730944Z","shell.execute_reply":"2025-01-16T08:55:11.743497Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load the `FLAN-T5` model ,creating an instance of the `AutoModelForSeq2SeqLM` class with the .from_pretrained() method","metadata":{}},{"cell_type":"markdown","source":"### loading model FLAN-T5","metadata":{}},{"cell_type":"code","source":"model_name = \"google/flan-t5-base\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T09:05:26.898763Z","iopub.execute_input":"2025-01-16T09:05:26.899230Z","iopub.status.idle":"2025-01-16T09:05:27.914168Z","shell.execute_reply.started":"2025-01-16T09:05:26.899197Z","shell.execute_reply":"2025-01-16T09:05:27.912767Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### loading Tokenization","metadata":{}},{"cell_type":"markdown","source":"To perform encoding and decoding , you need to work with text in a tokenized form. **Tokenizarion is the process of splitting texts into smaller units that can be pocessed by the LLM Model**.","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T09:05:42.340369Z","iopub.execute_input":"2025-01-16T09:05:42.340911Z","iopub.status.idle":"2025-01-16T09:05:42.639671Z","shell.execute_reply.started":"2025-01-16T09:05:42.340874Z","shell.execute_reply":"2025-01-16T09:05:42.638294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ninputs = tokenizer(\"Hey my name is Priyam!\", return_tensors=\"pt\")\noutputs = model.generate(**inputs)\nprint(\"Tokenized input: \")\nprint(outputs)\ndecoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"Decoded output: \")\nprint(decoded_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T09:07:15.288443Z","iopub.execute_input":"2025-01-16T09:07:15.288803Z","iopub.status.idle":"2025-01-16T09:07:15.629965Z","shell.execute_reply.started":"2025-01-16T09:07:15.288775Z","shell.execute_reply":"2025-01-16T09:07:15.629036Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now it's time to explore how well the base LLM summarizes a dialogue without any prompt engineering. Prompt engineering is an act of a human changing the prompt (input) to improve the response for a given task.","metadata":{}},{"cell_type":"code","source":"for i , index in enumerate(example_indices):\n    dialogue = test_ds[index]['dialogue']\n    summary = test_ds[index]['summary']\n\n    #1. tokenising the input sentence\n    inputs = tokenizer(dialogue ,return_tensors =\"pt\")\n    #2 . inferencing \n    model_predicted_tokens = model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens=50\n    )\n    model_predicted_tokens=model_predicted_tokens[0]\n    #3. Decoding the tokens that we got from the model\n    output = tokenizer.decode(\n        model_predicted_tokens,\n        skip_special_tokens=True\n    )\n\n    print_border_line()\n    print('Example ', i + 1)\n    print_border_line()\n    print(f'INPUT PROMPT:\\n{dialogue}')\n    print_border_line()\n    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n    print_border_line()\n    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T09:14:06.242459Z","iopub.execute_input":"2025-01-16T09:14:06.242965Z","iopub.status.idle":"2025-01-16T09:14:08.384464Z","shell.execute_reply.started":"2025-01-16T09:14:06.242923Z","shell.execute_reply":"2025-01-16T09:14:08.383493Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Ouputs of the model makes some sense , but it does not seem to be sure what task it is supposed to do. \n\nLets try see prompt engineering ...","metadata":{}},{"cell_type":"markdown","source":"## Summarize Dialogue with an Instruction Prompt","metadata":{}},{"cell_type":"markdown","source":"prompt engineering is an important concept in using foundation models for text generation . ","metadata":{}},{"cell_type":"markdown","source":"### Zero Shot inference with an Instruction prompt","metadata":{}},{"cell_type":"markdown","source":"Creating a sample zero shot inference Prompt","metadata":{}},{"cell_type":"code","source":"rndm_val =random.choice(test_ds)\ndialogue=rndm_val[\"dialogue\"]\nsummary = rndm_val[\"summary\"]\n\nprompt =f\"\"\"\nSummarize thr following conversation.\n\n{dialogue}\n\nSummary:\n    \n\"\"\"\nprint(prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T09:22:23.137835Z","iopub.execute_input":"2025-01-16T09:22:23.138303Z","iopub.status.idle":"2025-01-16T09:22:23.145524Z","shell.execute_reply.started":"2025-01-16T09:22:23.138271Z","shell.execute_reply":"2025-01-16T09:22:23.144399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i , index in enumerate(example_indices):\n    dialogue = test_ds[index]['dialogue']\n    summary = test_ds[index]['summary']\n\n    prompt= f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\n    \n    \"\"\"\n\n    \n    \n    inputs = tokenizer(prompt ,return_tensors =\"pt\")\n    #2 . inferencing \n    model_predicted_tokens = model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens=50\n    )\n    model_predicted_tokens=model_predicted_tokens[0]\n    #3. Decoding the tokens that we got from the model\n    output = tokenizer.decode(\n        model_predicted_tokens,\n        skip_special_tokens=True\n    )\n\n\n    print_border_line()\n    print('Example ', i + 1)\n    print_border_line()\n    print(f'INPUT PROMPT:\\n{dialogue}')\n    print_border_line()\n    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n    print_border_line()\n    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')\n\n    \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T09:24:10.177088Z","iopub.execute_input":"2025-01-16T09:24:10.177424Z","iopub.status.idle":"2025-01-16T09:24:12.112935Z","shell.execute_reply.started":"2025-01-16T09:24:10.177398Z","shell.execute_reply":"2025-01-16T09:24:12.111915Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Much better! But the model still does not pick up on the naunce of the conversations though","metadata":{}},{"cell_type":"markdown","source":"Experiment with the prompt text and see how the inferences will be changed. Will the inferences change if you end the prompt with just empty string vs. Summary: ?\nTry to rephrase the beginning of the prompt text from Summarize the following conversation. to something different - and see how it will influence the generated output.","metadata":{}},{"cell_type":"markdown","source":"### Zero shot inference with differebt Prompt Tempelate used","metadata":{}},{"cell_type":"code","source":"for i , index in enumerate(example_indices):\n    dialogue = test_ds[index]['dialogue']\n    summary = test_ds[index]['summary']\n\n    prompt = f\"\"\"\nDialogue:\n\n{dialogue}\n\nWhat was going on?\n\"\"\"\n\n    \n    \n    inputs = tokenizer(prompt ,return_tensors =\"pt\")\n    #2 . inferencing \n    model_predicted_tokens = model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens=50\n    )\n    model_predicted_tokens=model_predicted_tokens[0]\n    #3. Decoding the tokens that we got from the model\n    output = tokenizer.decode(\n        model_predicted_tokens,\n        skip_special_tokens=True\n    )\n\n\n    print_border_line()\n    print('Example ', i + 1)\n    print_border_line()\n    print(f'INPUT PROMPT:\\n{dialogue}')\n    print_border_line()\n    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n    print_border_line()\n    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')\n\n    \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T09:27:42.744946Z","iopub.execute_input":"2025-01-16T09:27:42.745419Z","iopub.status.idle":"2025-01-16T09:27:46.536423Z","shell.execute_reply.started":"2025-01-16T09:27:42.745380Z","shell.execute_reply":"2025-01-16T09:27:46.535457Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Notice that this prompt from FLAN-T5 did help a bit, but still struggles to pick up on the nuance of the conversation. This is what you will try to solve with the few shot inferencing.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### One Shot and Few Shot Inference","metadata":{}},{"cell_type":"markdown","source":"One shot and few shot inference are the practices of providing an LLM with either one or more full examples of prompt-response pairs that match your task - before your actual prompt that you want completed. This is called \"in-context learning\" and puts your model into a state that understands your specific task","metadata":{}},{"cell_type":"markdown","source":"Making prompt","metadata":{}},{"cell_type":"code","source":"def make_prompt(example_indices_full, example_index_to_summarize):\n    prompt = ''\n    \n    # Loop through the few-shot examples\n    for index in example_indices_full:\n        dialogue = test_ds[index][\"dialogue\"]\n        summary = test_ds[index][\"summary\"]\n\n        prompt += f\"\"\"\nDialogue:\n\n{dialogue}\n\nWhat was going on?\n{summary}\n    \n    \"\"\"\n    \n    # Add the example that needs to be summarized\n    dialogue = test_ds[example_index_to_summarize][\"dialogue\"]\n\n    prompt += f\"\"\"\nDialogue:\n\n{dialogue}\n\nWhat was going on?\n\n    \"\"\"\n\n    return prompt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T09:52:15.172351Z","iopub.execute_input":"2025-01-16T09:52:15.172777Z","iopub.status.idle":"2025-01-16T09:52:15.179463Z","shell.execute_reply.started":"2025-01-16T09:52:15.172745Z","shell.execute_reply":"2025-01-16T09:52:15.178350Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_indices_full = [40]\nexample_index_to_summarize = 200\n\none_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n\nprint(one_shot_prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T09:52:16.435807Z","iopub.execute_input":"2025-01-16T09:52:16.436256Z","iopub.status.idle":"2025-01-16T09:52:16.443885Z","shell.execute_reply.started":"2025-01-16T09:52:16.436225Z","shell.execute_reply":"2025-01-16T09:52:16.442365Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Prompt is Created , Now pass this promt to perform the `one shot inference`","metadata":{}},{"cell_type":"code","source":"summary = test_ds[example_index_to_summarize][\"summary\"]\n\n#1. Tokenizing the inpit sentences\ninputs =tokenizer(one_shot_prompt , return_tensors=\"pt\")\n\n#2. Passing it to the model\nmodel_output_tokenized = model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens=50,\n    )\n\noutput = tokenizer.decode(\n    model_output_tokenized[0],\n    skip_special_tokens=True\n)\nprint_border_line()\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint_border_line()\nprint(f'MODEL GENERATION - ONE SHOT:\\n{output}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T09:52:19.624063Z","iopub.execute_input":"2025-01-16T09:52:19.624498Z","iopub.status.idle":"2025-01-16T09:52:22.365495Z","shell.execute_reply.started":"2025-01-16T09:52:19.624467Z","shell.execute_reply":"2025-01-16T09:52:22.364387Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### FEW SHOT INFERENCE","metadata":{}},{"cell_type":"code","source":"example_indices_full = [40, 80, 120]\nexample_index_to_summarize = 200\n\nfew_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n\nprint(few_shot_prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T09:52:22.367001Z","iopub.execute_input":"2025-01-16T09:52:22.367388Z","iopub.status.idle":"2025-01-16T09:52:22.374125Z","shell.execute_reply.started":"2025-01-16T09:52:22.367347Z","shell.execute_reply":"2025-01-16T09:52:22.372639Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary = test_ds[example_index_to_summarize][\"summary\"]\n\n#1. Tokenizing the inpit sentences\ninputs =tokenizer(few_shot_prompt , return_tensors=\"pt\")\n\n#2. Passing it to the model\nmodel_output_tokenized = model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens=50,\n    )\n\noutput = tokenizer.decode(\n    model_output_tokenized[0],\n    skip_special_tokens=True\n)\nprint_border_line()\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint_border_line()\nprint(f'MODEL GENERATION - FEW SHOT:\\n{output}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T09:53:20.358312Z","iopub.execute_input":"2025-01-16T09:53:20.358765Z","iopub.status.idle":"2025-01-16T09:53:24.707276Z","shell.execute_reply.started":"2025-01-16T09:53:20.358730Z","shell.execute_reply":"2025-01-16T09:53:24.706148Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nIn this case, few shot did not provide much of an improvement over one shot inference. And, anything above 5 or 6 shot will typically not help much, either. Also, you need to make sure that you do not exceed the model's input-context length which, in our case, if 512 tokens. Anything above the context length will be ignored","metadata":{}},{"cell_type":"markdown","source":"However, you can see that feeding in at least one full example (one shot) provides the model with more information and qualitatively improves the summary overall.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generative Configuration Params for Inference","metadata":{}},{"cell_type":"markdown","source":"Earlier we were using generate() method to get output from LLM , and using only one params named `max_new_tokens=50` ,which means the maximum nnumber of tokens generated by the LLM coming out of the decoder model .\n\nA convenient way of organising the configuration params is to use `GenerationConfig` class","metadata":{}},{"cell_type":"markdown","source":"### Trying out different parameters","metadata":{}},{"cell_type":"markdown","source":"Currently here , I am enabling do_smaple =True and tweaking temperature value","metadata":{}},{"cell_type":"markdown","source":"Putting the parameter do_sample = True, you activate various decoding strategies which influence the next token from the probability distribution over the entire vocabulary. You can then adjust the outputs changing temperature and other parameters (such as top_k and top_p).","metadata":{}},{"cell_type":"code","source":"# generation_config = GenerationConfig(max_new_tokens=50)\n# generation_config = GenerationConfig(max_new_tokens=10)\n# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\ngeneration_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T10:00:28.217509Z","iopub.execute_input":"2025-01-16T10:00:28.218405Z","iopub.status.idle":"2025-01-16T10:00:28.225928Z","shell.execute_reply.started":"2025-01-16T10:00:28.218308Z","shell.execute_reply":"2025-01-16T10:00:28.224094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs = tokenizer(few_shot_prompt, return_tensors='pt')\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"],\n        generation_config=generation_config,\n    )[0], \n    skip_special_tokens=True\n)\n\n# print(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n# print(dash_line)\nprint(\"----------------------------------\")\nprint(f'MODEL GENERATION - FEW SHOT:\\n{output}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T10:00:28.859943Z","iopub.execute_input":"2025-01-16T10:00:28.860326Z","iopub.status.idle":"2025-01-16T10:00:32.010030Z","shell.execute_reply.started":"2025-01-16T10:00:28.860297Z","shell.execute_reply":"2025-01-16T10:00:32.008813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}